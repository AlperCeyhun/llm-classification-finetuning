{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T18:21:03.280932Z",
     "iopub.status.busy": "2025-05-26T18:21:03.280656Z",
     "iopub.status.idle": "2025-05-26T18:21:09.435312Z",
     "shell.execute_reply": "2025-05-26T18:21:09.434352Z",
     "shell.execute_reply.started": "2025-05-26T18:21:03.280904Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Data Complete\n"
     ]
    }
   ],
   "source": [
    "# import the data\n",
    "training = pd.read_csv('../../public/model/train.csv')\n",
    "test = pd.read_csv('../../public/model/test.csv')\n",
    "\n",
    "training['train_test'] = 1\n",
    "test['train_test'] = 0\n",
    "all_data = pd.concat([training,test])\n",
    "\n",
    "print(\"Import Data Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T18:30:44.748580Z",
     "iopub.status.busy": "2025-05-26T18:30:44.748242Z",
     "iopub.status.idle": "2025-05-26T18:30:44.774925Z",
     "shell.execute_reply": "2025-05-26T18:30:44.774076Z",
     "shell.execute_reply.started": "2025-05-26T18:30:44.748555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Handle missing values in text columns\n",
    "for col in [\"prompt\", \"response_a\", \"response_b\"]:\n",
    "    training[col] = training[col].fillna(\"\")\n",
    "    test[col] = test[col].fillna(\"\")\n",
    "\n",
    "# Add response_a_length and response_b_length columns\n",
    "training['response_a_length'] = training['response_a'].apply(len)\n",
    "training['response_b_length'] = training['response_b'].apply(len)\n",
    "test['response_a_length'] = test['response_a'].apply(len)\n",
    "test['response_b_length'] = test['response_b'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine model_a and model_b\n",
    "all_models = pd.concat([training['model_a'], training['model_b']])\n",
    "\n",
    "# Count occurrences of each model name across both columns\n",
    "model_counts = all_models.value_counts()\n",
    "print(\"\\nModel occurrence counts (across model_a and model_b):\")\n",
    "print(model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine model_a and model_b, count occurrences\n",
    "all_models = pd.concat([training['model_a'], training['model_b']])\n",
    "model_counts = all_models.value_counts()\n",
    "\n",
    "# Keep only models that occur at least 1000 times\n",
    "frequent_models = model_counts[model_counts >= 1000].index\n",
    "\n",
    "# Filter training data to keep only rows where both model_a and model_b are frequent\n",
    "training = training[\n",
    "    training['model_a'].isin(frequent_models) & training['model_b'].isin(frequent_models)\n",
    "]\n",
    "\n",
    "print(\"Filtered training shape:\", training.shape)\n",
    "print(\"Remaining unique models:\", len(frequent_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of each winner type in the training data\n",
    "num_a = training['winner_model_a'].sum()\n",
    "num_b = training['winner_model_b'].sum()\n",
    "num_tie = training['winner_tie'].sum()\n",
    "\n",
    "print(f\"winner_model_a: {num_a}\")\n",
    "print(f\"winner_model_b: {num_b}\")\n",
    "print(f\"winner_tie: {num_tie}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T18:31:14.265235Z",
     "iopub.status.busy": "2025-05-26T18:31:14.264914Z",
     "iopub.status.idle": "2025-05-26T18:31:53.546143Z",
     "shell.execute_reply": "2025-05-26T18:31:53.545190Z",
     "shell.execute_reply.started": "2025-05-26T18:31:14.265213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data handling\n",
    "# Vectorization of text data using TF-IDF\n",
    "vectorizer_prompt = TfidfVectorizer(max_features=150)\n",
    "vectorizer_response_a = TfidfVectorizer(max_features=150)\n",
    "vectorizer_response_b = TfidfVectorizer(max_features=150)\n",
    "\n",
    "# Fit and transform training data\n",
    "temp_prompt = vectorizer_prompt.fit_transform(training[\"prompt\"])\n",
    "temp_response_a = vectorizer_response_a.fit_transform(training[\"response_a\"])\n",
    "temp_response_b = vectorizer_response_b.fit_transform(training[\"response_b\"])\n",
    "\n",
    "# Print vector dimensions\n",
    "print(\"Number of elements for 'prompt':\", temp_prompt.shape)\n",
    "print(\"Number of elements for 'response a':\", temp_response_a.shape)\n",
    "print(\"Number of elements for 'response b':\", temp_response_b.shape)\n",
    "\n",
    "# Extract winner label from row\n",
    "def get_winner(row):\n",
    "    if row['winner_model_a'] == 1:\n",
    "        return 0\n",
    "    if row['winner_model_b'] == 1:\n",
    "        return 1\n",
    "    if row['winner_tie'] == 1:\n",
    "        return 2\n",
    "    return -1\n",
    "\n",
    "# Apply winner extraction\n",
    "training['winner'] = training.apply(get_winner, axis=1)\n",
    "train_y = training[\"winner\"].values\n",
    "\n",
    "# Concatenate feature arrays\n",
    "train_X = np.hstack([temp_prompt.toarray(), temp_response_a.toarray(), temp_response_b.toarray()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_X_train, train_X_val, train_y_train, train_y_val = train_test_split(\n",
    "    train_X, train_y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Train logistic regression model\n",
    "start = datetime.now()\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    learning_rate_init=0.001,  # Set initial learning rate\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10\n",
    ")\n",
    "model.fit(train_X_train, train_y_train)\n",
    "end = datetime.now()\n",
    "\n",
    "# Print execution time\n",
    "execution_time = (end - start).total_seconds() / 60\n",
    "print(f\"The time of execution is: {execution_time:.4f} minutes\")\n",
    "print(\"Model Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T18:35:22.964053Z",
     "iopub.status.busy": "2025-05-26T18:35:22.963631Z",
     "iopub.status.idle": "2025-05-26T18:35:23.180977Z",
     "shell.execute_reply": "2025-05-26T18:35:23.180061Z",
     "shell.execute_reply.started": "2025-05-26T18:35:22.964029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Predict class labels and probabilities on validation set\n",
    "start = datetime.now()\n",
    "value_y_predict = model.predict(train_X_val)\n",
    "value_y_probabilities = model.predict_proba(train_X_val)\n",
    "\n",
    "print(\"Prediction Probabilities (columns: A | B | Tie):\")\n",
    "print(value_y_probabilities)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "cm = confusion_matrix(train_y_val, value_y_predict)\n",
    "accuracy = model.score(train_X_val, train_y_val)\n",
    "macro_precision = precision_score(train_y_val, value_y_predict, average='macro')\n",
    "macro_recall = recall_score(train_y_val, value_y_predict, average='macro')\n",
    "micro_precision = precision_score(train_y_val, value_y_predict, average='micro')\n",
    "micro_recall = recall_score(train_y_val, value_y_predict, average='micro')\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['A wins', 'B wins', 'Tie'], \n",
    "            yticklabels=['A wins', 'B wins', 'Tie'])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "end = datetime.now()\n",
    "execution_time = (end - start).total_seconds()\n",
    "print(f\"\\nValidation Execution Time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T18:35:44.483802Z",
     "iopub.status.busy": "2025-05-26T18:35:44.483453Z",
     "iopub.status.idle": "2025-05-26T18:35:44.497113Z",
     "shell.execute_reply": "2025-05-26T18:35:44.496111Z",
     "shell.execute_reply.started": "2025-05-26T18:35:44.483772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_log_loss = log_loss(train_y_val, value_y_probabilities)\n",
    "print('Model Log loss:', model_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T18:35:57.337338Z",
     "iopub.status.busy": "2025-05-26T18:35:57.336921Z",
     "iopub.status.idle": "2025-05-26T18:35:57.352768Z",
     "shell.execute_reply": "2025-05-26T18:35:57.351799Z",
     "shell.execute_reply.started": "2025-05-26T18:35:57.337314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model prediction\n",
    "# transform test data using the trained vectorizers\n",
    "temp_test_prompt = vectorizer_prompt.transform(test[\"prompt\"])\n",
    "temp_test_response_a = vectorizer_response_a.transform(test[\"response_a\"])\n",
    "temp_test_response_b = vectorizer_response_b.transform(test[\"response_b\"])\n",
    "\n",
    "# concatenate all transformed fields to form the final test feature matrix\n",
    "test_X = np.hstack([\n",
    "    temp_test_prompt.toarray(),\n",
    "    temp_test_response_a.toarray(),\n",
    "    temp_test_response_b.toarray()\n",
    "])\n",
    "\n",
    "# predict class probabilities on the test data\n",
    "value_test_y_probabilities = model.predict_proba(test_X)\n",
    "\n",
    "# create a DataFrame for better visualization\n",
    "proba_df = pd.DataFrame(\n",
    "    np.round(value_test_y_probabilities, 4),\n",
    "    columns=[\"Prob_A_Wins\", \"Prob_B_Wins\", \"Prob_Tie\"]\n",
    ")\n",
    "\n",
    "print(\"Model Winner Prediction Probabilities:\\n\")\n",
    "display(proba_df)\n",
    "joblib.dump(model, 'mymodel.pkl')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9809560,
     "sourceId": 86518,
     "sourceType": "competition"
    },
    {
     "datasetId": 7419671,
     "sourceId": 11813172,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7419681,
     "sourceId": 11813182,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":11813172,"sourceType":"datasetVersion","datasetId":7419671},{"sourceId":11813182,"sourceType":"datasetVersion","datasetId":7419681}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import the data\nimport pandas as pd\n\ntraining = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\ntraining['train_test'] = 1\ntest['train_test'] = 0\nall_data = pd.concat([training,test])\n\nprint(\"Import Data Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:21:03.280656Z","iopub.execute_input":"2025-05-26T18:21:03.280932Z","iopub.status.idle":"2025-05-26T18:21:09.435312Z","shell.execute_reply.started":"2025-05-26T18:21:03.280904Z","shell.execute_reply":"2025-05-26T18:21:09.434352Z"}},"outputs":[{"name":"stdout","text":"Import Data Complete\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"training.head(10)\ntraining.tail(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:30:44.748242Z","iopub.execute_input":"2025-05-26T18:30:44.748580Z","iopub.status.idle":"2025-05-26T18:30:44.774925Z","shell.execute_reply.started":"2025-05-26T18:30:44.748555Z","shell.execute_reply":"2025-05-26T18:30:44.774076Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"               id             model_a             model_b  \\\n57467  4294254797  gpt-4-1106-preview          claude-2.0   \n57468  4294349169              palm-2         guanaco-33b   \n57469  4294452769  gpt-4-1106-preview        openchat-3.5   \n57470  4294602932          claude-2.1  gpt-4-1106-preview   \n57471  4294633312      pplx-7b-online          gpt-4-0613   \n57472  4294656694          gpt-4-0613            claude-1   \n57473  4294692063          claude-2.0    llama-2-13b-chat   \n57474  4294710549            claude-1          alpaca-13b   \n57475  4294899228              palm-2      tulu-2-dpo-70b   \n57476  4294947231  gemini-pro-dev-api  gpt-4-1106-preview   \n\n                                                  prompt  \\\n57467  [\"can you provide a modular python code that w...   \n57468  [\"Schreibe ein Bewerbungsanschreiben in deutsc...   \n57469  [\"Old McDonald harvested 4 pumpkins this year....   \n57470  [\"I want you to act like {character} from {ser...   \n57471                                            [\"abc\"]   \n57472  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n57473  [\"In python, implement a naive Bayes with gaus...   \n57474  [\"is it unethical to work on building weapons?...   \n57475  [\"If a bait contains 0,0025% bromadiolon then ...   \n57476  [\"three kids eat three apples in three days, h...   \n\n                                              response_a  \\\n57467  [\"Certainly! Below is a modular Python example...   \n57468  [\"I am trained to understand and respond only ...   \n57469  [\"Old McDonald originally harvested 4 pumpkins...   \n57470  [\"*responds in character's voice* Well hello t...   \n57471  [\"It seems like you're referring to the Califo...   \n57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n57473  [\"Here is an implementation of a naive Bayes c...   \n57474  [\"Working on weapons technology raises some et...   \n57475  [\"Bromadiolone is a rodenticide which is most ...   \n57476                                      [\"27 apples\"]   \n\n                                              response_b  winner_model_a  \\\n57467  [\"Here is a modular Python code to format SQL ...               1   \n57468  [\"Here is a possible Bewerbungsanschreiben in ...               0   \n57469  [\"Old McDonald had 4 pumpkins initially and no...               0   \n57470  [\"Of course! However, you'll need to specify w...               0   \n57471  [\"Hello! It seems like you've typed \\\"abc.\\\" H...               1   \n57472  [\"Here is how that mnemonic represents the dig...               1   \n57473  [\"Sure! Here's an implementation of a naive Ba...               1   \n57474  [\"It depends on the context. Weapons can be us...               1   \n57475  [\"As an AI language model, I do not promote or...               0   \n57476  [\"If three kids eat three apples in three days...               1   \n\n       winner_model_b  winner_tie  train_test  \n57467               0           0           1  \n57468               1           0           1  \n57469               0           1           1  \n57470               1           0           1  \n57471               0           0           1  \n57472               0           0           1  \n57473               0           0           1  \n57474               0           0           1  \n57475               1           0           1  \n57476               0           0           1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n      <th>train_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>57467</th>\n      <td>4294254797</td>\n      <td>gpt-4-1106-preview</td>\n      <td>claude-2.0</td>\n      <td>[\"can you provide a modular python code that w...</td>\n      <td>[\"Certainly! Below is a modular Python example...</td>\n      <td>[\"Here is a modular Python code to format SQL ...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>57468</th>\n      <td>4294349169</td>\n      <td>palm-2</td>\n      <td>guanaco-33b</td>\n      <td>[\"Schreibe ein Bewerbungsanschreiben in deutsc...</td>\n      <td>[\"I am trained to understand and respond only ...</td>\n      <td>[\"Here is a possible Bewerbungsanschreiben in ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>57469</th>\n      <td>4294452769</td>\n      <td>gpt-4-1106-preview</td>\n      <td>openchat-3.5</td>\n      <td>[\"Old McDonald harvested 4 pumpkins this year....</td>\n      <td>[\"Old McDonald originally harvested 4 pumpkins...</td>\n      <td>[\"Old McDonald had 4 pumpkins initially and no...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>57470</th>\n      <td>4294602932</td>\n      <td>claude-2.1</td>\n      <td>gpt-4-1106-preview</td>\n      <td>[\"I want you to act like {character} from {ser...</td>\n      <td>[\"*responds in character's voice* Well hello t...</td>\n      <td>[\"Of course! However, you'll need to specify w...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>57471</th>\n      <td>4294633312</td>\n      <td>pplx-7b-online</td>\n      <td>gpt-4-0613</td>\n      <td>[\"abc\"]</td>\n      <td>[\"It seems like you're referring to the Califo...</td>\n      <td>[\"Hello! It seems like you've typed \\\"abc.\\\" H...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>4294656694</td>\n      <td>gpt-4-0613</td>\n      <td>claude-1</td>\n      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n      <td>[\"Here is how that mnemonic represents the dig...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>4294692063</td>\n      <td>claude-2.0</td>\n      <td>llama-2-13b-chat</td>\n      <td>[\"In python, implement a naive Bayes with gaus...</td>\n      <td>[\"Here is an implementation of a naive Bayes c...</td>\n      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>4294710549</td>\n      <td>claude-1</td>\n      <td>alpaca-13b</td>\n      <td>[\"is it unethical to work on building weapons?...</td>\n      <td>[\"Working on weapons technology raises some et...</td>\n      <td>[\"It depends on the context. Weapons can be us...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>4294899228</td>\n      <td>palm-2</td>\n      <td>tulu-2-dpo-70b</td>\n      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n      <td>[\"As an AI language model, I do not promote or...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>4294947231</td>\n      <td>gemini-pro-dev-api</td>\n      <td>gpt-4-1106-preview</td>\n      <td>[\"three kids eat three apples in three days, h...</td>\n      <td>[\"27 apples\"]</td>\n      <td>[\"If three kids eat three apples in three days...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\n# Separate vectorizers for each field\nvectorizer_prompt = TfidfVectorizer(max_features=150)\nvectorizer_response_a = TfidfVectorizer(max_features=150)\nvectorizer_response_b = TfidfVectorizer(max_features=150)\n\n# Fit on training data\ntemp_prompt = vectorizer_prompt.fit_transform(training[\"prompt\"])\ntemp_response_a = vectorizer_response_a.fit_transform(training[\"response_a\"])\ntemp_response_b = vectorizer_response_b.fit_transform(training[\"response_b\"])\n\nprint(\"TF-IDF Features for Prompt:\", vectorizer_prompt.get_feature_names_out())\nprint(\"TF-IDF Features for Response A:\", vectorizer_response_a.get_feature_names_out())\nprint(\"TF-IDF Features for Response B:\", vectorizer_response_b.get_feature_names_out())\n\nprint(\"Number of elements for the vectorizer representation for 'prompt':\", temp_prompt.shape)\nprint(\"Number of elements for the vectorizer representation for 'response a':\", temp_response_a.shape)\nprint(\"Number of elements for the vectorizer representation for 'response b':\", temp_response_b.shape)\n\n#selecting the prediction target\ndef get_winner(row):\n    if row['winner_model_a'] == 1:\n        return 0  # response_a wins\n    elif row['winner_model_b'] == 1:\n        return 1  # response_b wins\n    elif row['winner_tie'] == 1:\n        return 2  # tie\n    else:\n        return -1  # unknown or missing\n\ntraining['winner'] = training.apply(get_winner, axis=1)\ntrain_y = training[\"winner\"].values\n\n#choosing \"features\"\ntrain_X = np.concatenate((temp_prompt.toarray(), temp_response_a.toarray(), temp_response_b.toarray()), axis=1)\n\nprint(\"Selecting The Prediction Target and Choosing Features Complete\")\n#use Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom datetime import datetime\n\n#record start time to calculate the execution time\nstart = datetime.now()\n\n#Logistic Regression\nmodel = LogisticRegression(max_iter=500, multi_class='multinomial', solver='saga') #For large datasets the “saga” solver is usually faster [scikit-learn documentation]\nmodel.fit(train_X, train_y)\n\n#record end time\nend = datetime.now()\n \n#calculate the execution time\nexecution_time = (end - start).total_seconds() / 60\nprint(f\"The time of execution is: {execution_time} minutes\")\n\n\nprint(\"Model Training Complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:31:14.264914Z","iopub.execute_input":"2025-05-26T18:31:14.265235Z","iopub.status.idle":"2025-05-26T18:31:53.546143Z","shell.execute_reply.started":"2025-05-26T18:31:14.265213Z","shell.execute_reply":"2025-05-26T18:31:53.545190Z"}},"outputs":[{"name":"stdout","text":"TF-IDF Features for Prompt: ['10' 'about' 'after' 'ai' 'all' 'also' 'am' 'an' 'and' 'answer' 'any'\n 'are' 'as' 'at' 'based' 'be' 'been' 'being' 'best' 'between' 'but' 'by'\n 'can' 'code' 'comment' 'could' 'create' 'data' 'day' 'do' 'does' 'don'\n 'each' 'example' 'explain' 'first' 'following' 'for' 'from' 'game' 'get'\n 'give' 'good' 'had' 'has' 'have' 'he' 'help' 'her' 'his' 'how' 'if' 'in'\n 'information' 'int' 'into' 'is' 'it' 'its' 'just' 'know' 'like' 'list'\n 'long' 'make' 'many' 'me' 'model' 'more' 'most' 'must' 'my' 'name' 'need'\n 'new' 'no' 'not' 'now' 'nthe' 'number' 'of' 'on' 'one' 'only' 'or'\n 'other' 'our' 'out' 'people' 'please' 'provide' 'python' 'question'\n 'return' 'self' 'she' 'should' 'so' 'some' 'step' 'story' 'such' 'system'\n 't0' 'take' 'tell' 'text' 'than' 'that' 'the' 'their' 'them' 'then'\n 'there' 'these' 'they' 'think' 'this' 'time' 'to' 'two' 'u2019s' 'u201d'\n 'up' 'use' 'user' 'using' 'very' 'want' 'was' 'way' 'we' 'were' 'what'\n 'when' 'where' 'which' 'who' 'why' 'will' 'with' 'word' 'words' 'work'\n 'world' 'would' 'write' 'year' 'you' 'your']\nTF-IDF Features for Response A: ['10' 'about' 'ai' 'all' 'also' 'an' 'and' 'any' 'are' 'as' 'at' 'based'\n 'be' 'been' 'being' 'between' 'both' 'but' 'by' 'can' 'code' 'could'\n 'create' 'data' 'different' 'do' 'each' 'example' 'file' 'find' 'first'\n 'for' 'from' 'function' 'game' 'get' 'had' 'has' 'have' 'he' 'help' 'her'\n 'here' 'high' 'his' 'how' 'however' 'if' 'important' 'in' 'include'\n 'including' 'information' 'into' 'is' 'it' 'its' 'just' 'key' 'know'\n 'known' 'language' 'let' 'life' 'like' 'make' 'many' 'may' 'me' 'might'\n 'model' 'more' 'most' 'my' 'n1' 'n2' 'n3' 'n4' 'n5' 'n6' 'name' 'need'\n 'new' 'nin' 'no' 'not' 'nthe' 'number' 'of' 'often' 'on' 'one' 'or'\n 'other' 'our' 'out' 'over' 'people' 'process' 'provide' 'python' 're'\n 'set' 'she' 'should' 'so' 'some' 'specific' 'such' 'support' 'system'\n 'take' 'than' 'that' 'the' 'their' 'them' 'then' 'there' 'these' 'they'\n 'this' 'through' 'time' 'to' 'two' 'u043e' 'up' 'use' 'used' 'using'\n 'value' 'was' 'way' 'we' 'well' 'were' 'what' 'when' 'where' 'which'\n 'while' 'who' 'will' 'with' 'work' 'world' 'would' 'you' 'your']\nTF-IDF Features for Response B: ['10' 'about' 'ai' 'all' 'also' 'an' 'and' 'any' 'are' 'as' 'at' 'based'\n 'be' 'been' 'being' 'between' 'both' 'but' 'by' 'can' 'code' 'could'\n 'create' 'data' 'different' 'do' 'each' 'example' 'find' 'first' 'for'\n 'from' 'function' 'game' 'get' 'had' 'has' 'have' 'he' 'help' 'her'\n 'here' 'high' 'his' 'how' 'however' 'if' 'important' 'in' 'include'\n 'including' 'information' 'into' 'is' 'it' 'its' 'just' 'key' 'know'\n 'known' 'language' 'let' 'life' 'like' 'make' 'many' 'may' 'me' 'might'\n 'model' 'more' 'most' 'my' 'n1' 'n2' 'n3' 'n4' 'n5' 'n6' 'name' 'need'\n 'new' 'nin' 'no' 'not' 'nthe' 'number' 'of' 'often' 'on' 'one' 'or'\n 'other' 'our' 'out' 'over' 'people' 'process' 'provide' 'python' 're'\n 'self' 'set' 'she' 'should' 'so' 'some' 'specific' 'such' 'sure' 'system'\n 'take' 'than' 'that' 'the' 'their' 'them' 'then' 'there' 'these' 'they'\n 'this' 'through' 'time' 'to' 'two' 'u043e' 'up' 'use' 'used' 'using'\n 'value' 'was' 'way' 'we' 'well' 'were' 'what' 'when' 'where' 'which'\n 'while' 'who' 'will' 'with' 'work' 'world' 'would' 'you' 'your']\nNumber of elements for the vectorizer representation for 'prompt': (57477, 150)\nNumber of elements for the vectorizer representation for 'response a': (57477, 150)\nNumber of elements for the vectorizer representation for 'response b': (57477, 150)\nSelecting The Prediction Target and Choosing Features Complete\nThe time of execution is: 0.28497355 minutes\nModel Training Complete\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\n\n#split into validation and training data\ntrain_X_train, train_X_val, train_y_train, train_y_val = train_test_split(train_X, train_y, test_size=0.2, random_state=42)\n\n#record start time to calculate the execution time\nstart = datetime.now()\n\n#think about results - comparing predictions (value_y_predict) to the actual winner model (train_y_val)\nvalue_y_predict = model.predict(train_X_val)\nprint('Model winner prediction', value_y_predict)\nprint('Model winner real value', train_y_val)\n\nvalue_y_probabilities = model.predict_proba(train_X_val)\nprint('Model winner prediction, probability', value_y_probabilities) #winner model a | winner model b | winner tie\n\n#confusion matrix\ncm = confusion_matrix(train_y_val, value_y_predict)\nprint(\"Confusion Matrix:\\n\", cm)\n\n#model accuracy\nscore = model.score(train_X_val, train_y_val)\nprint('Model Accuracy Score', score)\n\n#macro and micro averaged Precision and Recall\nmacro_precision = precision_score(train_y_val, value_y_predict, average='macro') #calculate precision for all classes individually and then average them\nmacro_recall = recall_score(train_y_val, value_y_predict, average='macro')\nmicro_precision = precision_score(train_y_val, value_y_predict, average='micro') #calculate class wise true positive and false positive and then use that to calculate overall precision\nmicro_recall = recall_score(train_y_val, value_y_predict, average='micro')\nprint(\"Macro Precision:\", macro_precision)\nprint(\"Macro Recall:\", macro_recall)\nprint(\"Micro Precision:\", micro_precision)\nprint(\"Micro Recall:\", micro_recall)\n\n#record end time\nend = datetime.now()\n \n#calculate the execution time\nexecution_time = (end - start).total_seconds()\nprint(f\"The time of execution is: {execution_time} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:35:22.963631Z","iopub.execute_input":"2025-05-26T18:35:22.964053Z","iopub.status.idle":"2025-05-26T18:35:23.180977Z","shell.execute_reply.started":"2025-05-26T18:35:22.964029Z","shell.execute_reply":"2025-05-26T18:35:23.180061Z"}},"outputs":[{"name":"stdout","text":"Model winner prediction [0 1 2 ... 1 0 0]\nModel winner real value [0 0 2 ... 1 0 0]\nModel winner prediction, probability [[0.41029326 0.34035515 0.24935159]\n [0.21808727 0.54393733 0.2379754 ]\n [0.28465304 0.26706572 0.44828124]\n ...\n [0.26382212 0.49364626 0.24253162]\n [0.40534063 0.30900811 0.28565126]\n [0.54751897 0.17925463 0.2732264 ]]\nConfusion Matrix:\n [[2128 1186  716]\n [1267 1990  672]\n [1228 1152 1157]]\nModel Accuracy Score 0.45885525400139177\nMacro Precision: 0.45824024285175624\nMacro Recall: 0.4538810920723748\nMicro Precision: 0.45885525400139177\nMicro Recall: 0.45885525400139177\nThe time of execution is: 0.126692 seconds\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#model log loss - https://www.kaggle.com/competitions/llm-classification-finetuning/discussion/552103\nfrom sklearn.metrics import log_loss\n\nmodel_log_loss = log_loss(train_y_val, value_y_probabilities)\n\nprint('Model Log loss:', model_log_loss) \n\n# Number of classes = 3 : Logloss = - log(1/3) = 1.10\n# Model Log loss: 1.05, model prediction is considered good for this project","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:35:44.483453Z","iopub.execute_input":"2025-05-26T18:35:44.483802Z","iopub.status.idle":"2025-05-26T18:35:44.497113Z","shell.execute_reply.started":"2025-05-26T18:35:44.483772Z","shell.execute_reply":"2025-05-26T18:35:44.496111Z"}},"outputs":[{"name":"stdout","text":"Model Log loss: 1.0465078397010954\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#model prediction\n# transform test data using the trained vectorizers\ntemp_test_prompt = vectorizer_prompt.transform(test[\"prompt\"])\ntemp_test_response_a = vectorizer_response_a.transform(test[\"response_a\"])\ntemp_test_response_b = vectorizer_response_b.transform(test[\"response_b\"])\n\ntest_X = np.concatenate((temp_test_prompt.toarray(), temp_test_response_a.toarray(), temp_test_response_b.toarray()), axis=1)\nvalue_test_y_probabilities = model.predict_proba(test_X)\nprint('Model winner prediction, probability', value_test_y_probabilities) #winner model a | winner model b | winner tie","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:35:57.336921Z","iopub.execute_input":"2025-05-26T18:35:57.337338Z","iopub.status.idle":"2025-05-26T18:35:57.352768Z","shell.execute_reply.started":"2025-05-26T18:35:57.337314Z","shell.execute_reply":"2025-05-26T18:35:57.351799Z"}},"outputs":[{"name":"stdout","text":"Model winner prediction, probability [[0.21811121 0.20748486 0.57440394]\n [0.45421621 0.26237458 0.2834092 ]\n [0.26653411 0.38061243 0.35285347]]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"output = pd.DataFrame({'id': test.id,\n                        'winner_model_a': value_test_y_probabilities[:, 0],\n                        'winner_model_b': value_test_y_probabilities[:, 1],\n                        'winner_tie': value_test_y_probabilities[:, 2]})\noutput.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T18:35:59.420745Z","iopub.execute_input":"2025-05-26T18:35:59.421023Z","iopub.status.idle":"2025-05-26T18:35:59.431993Z","shell.execute_reply.started":"2025-05-26T18:35:59.421004Z","shell.execute_reply":"2025-05-26T18:35:59.431161Z"}},"outputs":[],"execution_count":7}]}